# -*- coding: utf-8 -*-
"""eye_detection.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1YToz4ov75ltcX1xHXWuP4Oajsy38fMkS
"""

from shutil import copyfile
import pandas as pd
import torch
import torch.nn as nn
import torchvision.models as models
import torchvision.transforms as transforms
from torch.utils.data import Dataset, DataLoader
from PIL import Image
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score
import cv2
import torch.optim as optim
import os
from google.colab import drive
drive.mount('/content/drive')
zip_path = '/content/drive/MyDrive/KhudaCV/CV.zip'

import zipfile

# 압축 해제 경로
extract_path = '/content/CV'
os.makedirs(extract_path, exist_ok=True)

# 압축 해제
with zipfile.ZipFile(zip_path, 'r') as zip_ref:
    zip_ref.extractall(extract_path)

# 압축 해제된 파일 경로
label_file = os.path.join(extract_path, 'list_attr_celeba.csv')
img_folder = os.path.join(extract_path, 'img_align_celeba/img_align_celeba')

# 경로 출력 확인
print(f"Label file: {label_file}")
print(f"Image folder: {img_folder}")

# CSV 파일 읽기
attr_df = pd.read_csv(label_file)
print(attr_df.head())
print(f"Number of images: {len(os.listdir(img_folder))}")

# 저장 폴더 생성
os.makedirs('data/train/closed', exist_ok=True)
os.makedirs('data/train/open', exist_ok=True)

# 눈 감김 상태로 이미지 분류 (예: 'Eyeglasses' 속성으로 분류)
for _, row in attr_df.iterrows():
    img_name = row['image_id']

    # 'Eyeglasses' 속성이 -1이면 눈 감김, 1이면 눈 뜬 상태로 분류
    if row['Eyeglasses'] == -1:
        dest_folder = 'data/train/closed'
    else:
        dest_folder = 'data/train/open'

    src_path = os.path.join(img_folder, img_name)
    dest_path = os.path.join(dest_folder, img_name)

    # 디버깅용 경로 출력
    print(f"파일 경로: {src_path}")

    # 예외 처리를 추가하여 파일이 없으면 건너뛰기
    try:
        copyfile(src_path, dest_path)
    except FileNotFoundError:
        print(f"파일을 찾을 수 없습니다: {src_path}")

# Step 2: 데이터셋 클래스 정의
from sklearn.model_selection import train_test_split
from PIL import Image
import numpy as np

class EyeStateDataset(Dataset):
    def __init__(self, image_paths, labels, transform=None, height=128, width=128):
        self.image_paths = image_paths
        self.labels = labels
        self.transform = transform
        self.height = height
        self.width = width

    def __len__(self):
        return len(self.image_paths)

    def __getitem__(self, idx):
        img_path = self.image_paths[idx]
        label = self.labels[idx]

        # 이미지 전처리
        image = self.preprocess(img_path, self.width, self.height)

        return image, label

    @staticmethod
    def preprocess(image_path, w, h):
        image = Image.open(image_path).convert("RGB")
        x, y = image.size

        # 중심부 자르기와 크기 조정
        image = transforms.CenterCrop(min(x, y))(image)
        image = image.resize((w, h), resample=Image.Resampling.LANCZOS)

        # NumPy로 변환 및 정규화
        image = np.array(image).astype(np.float32)
        image = EyeStateDataset.normalize(image)

        return image

    @staticmethod
    def normalize(image):
        # [-1, 1] 범위로 정규화 후 텐서 변환
        image = image / 127.5 - 1
        image = torch.tensor(image).permute(2, 0, 1)

        return image

# 데이터 경로 설정
root_dir = 'data/train'

# 전체 데이터 로드
all_image_paths = []
all_labels = []

for label, sub_dir in enumerate(['closed', 'open']):
    folder = os.path.join(root_dir, sub_dir)
    for img_name in os.listdir(folder):
        all_image_paths.append(os.path.join(folder, img_name))
        all_labels.append(label)

# 데이터셋 나누기 (test 비율: 0.33)
train_paths, test_paths, train_labels, test_labels = train_test_split(
    all_image_paths, all_labels, test_size=0.33, random_state=42
)

# train/test 데이터셋 생성
train_dataset = EyeStateDataset(train_paths, train_labels)
test_dataset = EyeStateDataset(test_paths, test_labels)

# 데이터로더 생성
train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)
test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)

# 데이터 확인
print(f"Train dataset size: {len(train_dataset)}")
print(f"Test dataset size: {len(test_dataset)}")

# Step 3: 모델 정의 (MobileNet을 기반으로 한 전이 학습 모델)
class EyeStateModel(nn.Module):
    def __init__(self):
        super(EyeStateModel, self).__init__()
        self.mobilenet = models.mobilenet_v2(pretrained=True)
        self.mobilenet.classifier[1] = nn.Linear(self.mobilenet.last_channel, 2)

    def forward(self, x):
        return self.mobilenet(x)

# 모델 초기화
device = 'cuda' if torch.cuda.is_available() else 'cpu'
model = EyeStateModel().to(device)

# Step 4: 모델 학습
criterion = nn.CrossEntropyLoss()
optimizer = torch.optim.Adam(model.parameters(), lr=0.001)
num_epochs = 5

# 가중치 저장 경로
save_path = 'model_weights'
os.makedirs(save_path, exist_ok=True)

for epoch in range(num_epochs):
    model.train()
    running_loss = 0.0
    total_correct = 0
    total_samples = 0

    for batch_idx, (images, labels) in enumerate(train_loader):
        images, labels = images.to(device), labels.to(device)

        # 모델 학습
        optimizer.zero_grad()
        outputs = model(images)
        loss = criterion(outputs, labels)
        loss.backward()
        optimizer.step()

        running_loss += loss.item()

        # Accuracy 계산
        _, preds = torch.max(outputs, 1)
        total_correct += (preds == labels).sum().item()
        total_samples += labels.size(0)

        # Index 10마다 손실과 정확도 출력
        if (batch_idx + 1) % 10 == 0:
            current_loss = running_loss / (batch_idx + 1)
            current_accuracy = total_correct / total_samples * 100
            print(f'Epoch {epoch+1}/{num_epochs}, Batch {batch_idx+1}/{len(train_loader)}, '
                  f'Loss: {current_loss:.4f}, Accuracy: {current_accuracy:.2f}%')

        # 배치 사이즈 200마다 모델 가중치 저장
        if (batch_idx + 1) % 200 == 0:
            torch.save(model.state_dict(), os.path.join(save_path, f'model_epoch{epoch+1}_batch{batch_idx+1}.pth'))
            print(f"Saved model weights at Epoch {epoch+1}, Batch {batch_idx+1}")

    # 에포크별 평균 손실 출력
    epoch_loss = running_loss / len(train_loader)
    epoch_accuracy = total_correct / total_samples * 100
    print(f'Epoch {epoch+1}/{num_epochs}, Average Loss: {epoch_loss:.4f}, Average Accuracy: {epoch_accuracy:.2f}%')

# Step 5: 모델 평가
model.eval()
y_true = []
y_pred = []

with torch.no_grad():
    for images, labels in test_loader:
        images = images.to(device)
        outputs = model(images)
        _, preds = torch.max(outputs, 1)

        y_true.extend(labels.numpy())
        y_pred.extend(preds.cpu().numpy())

# 평가 결과 계산
accuracy = accuracy_score(y_true, y_pred)
precision = precision_score(y_true, y_pred)
recall = recall_score(y_true, y_pred)
f1 = f1_score(y_true, y_pred)

print(f"Accuracy: {accuracy}")
print(f"Precision: {precision}")
print(f"Recall: {recall}")
print(f"F1 Score: {f1}")

"""여기 아래는 아직 학습 안 시킴"""

# Step 6: 실시간 눈 감김 감지 및 알림 시스템
cap = cv2.VideoCapture(0)  # 웹캠 연결
model.eval()

with torch.no_grad():
    while cap.isOpened():
        ret, frame = cap.read()
        if not ret:
            break

        # 얼굴 영역 감지 및 눈 상태 판단
        # [참고: YOLO나 다른 얼굴 검출 모델 사용 필요]

        # 프레임을 모델 입력 형태로 변환
        eye_image = cv2.resize(frame, (128, 128))
        eye_image = transforms.ToTensor()(eye_image).unsqueeze(0).to(device)

        # 눈 상태 예측
        outputs = model(eye_image)
        _, pred = torch.max(outputs, 1)
        eye_state = 'Closed' if pred.item() == 0 else 'Open'

        # 결과 출력
        cv2.putText(frame, f"Eye State: {eye_state}", (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 0, 0), 2)
        cv2.imshow('Eye State Detection', frame)

        if cv2.waitKey(1) & 0xFF == ord('q'):
            break

cap.release()
cv2.destroyAllWindows()

import cv2

# Haar Cascade 파일 로드
face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')
eye_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_eye.xml')

# 웹캠 연결 또는 이미지 로드
cap = cv2.VideoCapture(0)  # 0번 카메라 연결
# image = cv2.imread('your_image_path.jpg')  # 이미지를 사용하려면 주석 해제

while cap.isOpened():
    ret, frame = cap.read()
    if not ret:
        break

    # 흑백 변환 (Cascade는 흑백 이미지에서 동작)
    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)

    # 얼굴 검출
    faces = face_cascade.detectMultiScale(gray, scaleFactor=1.1, minNeighbors=5, minSize=(100, 100))

    for (x, y, w, h) in faces:
        # 얼굴 영역 자르기
        face_gray = gray[y:y+h, x:x+w]
        face_color = frame[y:y+h, x:x+w]

        # 얼굴 영역에서 눈 검출
        eyes = eye_cascade.detectMultiScale(face_gray, scaleFactor=1.1, minNeighbors=10, minSize=(30, 30))
        for (ex, ey, ew, eh) in eyes:
            # 눈 영역 자르기
            eye_image = face_color[ey:ey+eh, ex:ex+ew]

            # 눈 영역 전처리
            eye_image_resized = cv2.resize(eye_image, (128, 128))

            # 시각화용 바운딩 박스 표시
            cv2.rectangle(face_color, (ex, ey), (ex+ew, ey+eh), (0, 255, 0), 2)

    # 결과 시각화
    cv2.imshow('Multiple Eyes Detection (OpenCV)', frame)

    if cv2.waitKey(1) & 0xFF == ord('q'):
        break

cap.release()
cv2.destroyAllWindows()

import torch
import torch.nn as nn
import torch.optim as optim
from sklearn.metrics import accuracy_score

torch.save(model.state_dict(), 'eyeopen.pth')