# -*- coding: utf-8 -*-
"""eye_detection_arcface.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1q1Wd0WuiKofRA1Tp-bql2wNatlFVZiNR
"""

from shutil import copyfile
import pandas as pd
import torch
import torch.nn as nn
import torchvision.models as models
import torchvision.transforms as transforms
from torch.utils.data import Dataset, DataLoader
from PIL import Image
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score
import torch.nn.functional as F
import os
from sklearn.model_selection import train_test_split
import numpy as np
import zipfile
from google.colab import drive

# Colab Drive Mount
drive.mount('/content/drive')
zip_path = '/content/drive/MyDrive/KhudaCV/CV.zip'

# 압축 해제 경로
extract_path = '/content/CV'
os.makedirs(extract_path, exist_ok=True)

# 압축 해제
with zipfile.ZipFile(zip_path, 'r') as zip_ref:
    zip_ref.extractall(extract_path)

# 압축 해제된 파일 경로
label_file = os.path.join(extract_path, 'list_attr_celeba.csv')
img_folder = os.path.join(extract_path, 'img_align_celeba/img_align_celeba')

# CSV 파일 읽기
attr_df = pd.read_csv(label_file)
print(attr_df.head())
print(f"Number of images: {len(os.listdir(img_folder))}")

# 저장 폴더 생성
os.makedirs('data/train/closed', exist_ok=True)
os.makedirs('data/train/open', exist_ok=True)

# 눈 감김 상태로 이미지 분류
for _, row in attr_df.iterrows():
    img_name = row['image_id']
    if row['Eyeglasses'] == -1:
        dest_folder = 'data/train/closed'
    else:
        dest_folder = 'data/train/open'
    src_path = os.path.join(img_folder, img_name)
    dest_path = os.path.join(dest_folder, img_name)
    try:
        copyfile(src_path, dest_path)
    except FileNotFoundError:
        print(f"파일을 찾을 수 없습니다: {src_path}")

# Step 2: 데이터셋 클래스 정의
class EyeStateDataset(Dataset):
    def __init__(self, image_paths, labels, transform=None, height=128, width=128):
        self.image_paths = image_paths
        self.labels = labels
        self.transform = transform
        self.height = height
        self.width = width

    def __len__(self):
        return len(self.image_paths)

    def __getitem__(self, idx):
        img_path = self.image_paths[idx]
        label = self.labels[idx]
        image = self.preprocess(img_path, self.width, self.height)
        return image, label

    @staticmethod
    def preprocess(image_path, w, h):
        image = Image.open(image_path).convert("RGB")
        x, y = image.size
        image = image.resize((w, h), resample=Image.Resampling.LANCZOS)
        image = np.array(image).astype(np.float32)
        image = EyeStateDataset.normalize(image)
        return image

    @staticmethod
    def normalize(image):
        image = image / 127.5 - 1
        image = torch.tensor(image).permute(2, 0, 1)
        return image

# 데이터 경로 설정
root_dir = 'data/train'
all_image_paths = []
all_labels = []

for label, sub_dir in enumerate(['closed', 'open']):
    folder = os.path.join(root_dir, sub_dir)
    for img_name in os.listdir(folder):
        all_image_paths.append(os.path.join(folder, img_name))
        all_labels.append(label)

# 데이터셋 나누기
train_paths, test_paths, train_labels, test_labels = train_test_split(
    all_image_paths, all_labels, test_size=0.33, random_state=42
)

train_dataset = EyeStateDataset(train_paths, train_labels)
test_dataset = EyeStateDataset(test_paths, test_labels)

train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)
test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)

print(f"Train dataset size: {len(train_dataset)}")
print(f"Test dataset size: {len(test_dataset)}")

# Step 3: ArcFace 정의
class CosFace(nn.Module):
    def __init__(self, in_dim, out_dim, s, m):
        super(CosFace, self).__init__()
        self.s = s
        self.m = m
        self.W = nn.Parameter(torch.empty(in_dim, out_dim))

        nn.init.kaiming_uniform_(self.W)

    def forward(self, x):
        normalized_x = F.normalize(x, p=2, dim=1)
        normalized_W = F.normalize(self.W, p=2, dim=0)

        cosine = torch.matmul(normalized_x.view(normalized_x.size(0), -1), normalized_W)

        # Using torch.clamp() to ensure cosine values are within a safe range,
        # preventing potential NaN losses.

        theta = torch.acos(torch.clamp(cosine, -1.0 + 1e-7, 1.0 - 1e-7))

        probability = self.s * torch.cos(theta+self.m)

        return probability

# Step 4: MobileNet + ArcFace 모델 정의
# s: 각도 기반의 스케일링 매개변수, m: 마진 매개변수
class EyeStateModelWithArcFace(nn.Module):
    def __init__(self):
        super(EyeStateModelWithArcFace, self).__init__()
        self.mobilenet = models.mobilenet_v2(pretrained=True)
        self.mobilenet = nn.Sequential(*list(self.mobilenet.children())[:-1])  # 분류기 제거

        self.cosface = CosFace(in_dim=20480, out_dim=2, s=64.0, m=0.6)

    def forward(self, x, labels=None):
        features = self.mobilenet(x)
        features = features.view(features.size(0), -1)  # Flatten the features
        # print(f"Shape of MobileNet features: {features.shape}")  # 디버깅용 출력
        if labels is not None:
            output = self.cosface(features)
            return output
        else:
            return features


# 모델 초기화
device = 'cuda' if torch.cuda.is_available() else 'cpu'
model = EyeStateModelWithArcFace().to(device)

# print(self.mobilenet(x).shape)
# Shape of MobileNet features: torch.Size([32, 2])

# 1. 현재 구조
# torch.Size([32, 2])라는 출력은 MobileNet의 마지막 출력 계층이 2개 클래스 (open/closed)를 예측하도록 조정되어 있기 때문
# 따라서, 현재 MobileNet은 출력이 직접 2차원으로 나타나며, 이는 분류 레이어를 이미 포함한 상태입니다.

# 2. CosFace와의 연결 문제
# CosFace는 특징 벡터(예: 128, 256, 1280 차원 등)를 입력으로 받아야 하며, 이를 기반으로 클래스 간 각도를 학습합니다.
# 그러나 MobileNet 출력이 이미 2차원(클래스 개수)로 설정되어 있어, CosFace를 사용하려면 MobileNet에서 **분류기(classifier)**를 제거하고, 특징 벡터만 반환하도록 해야 합니다.

# Step 4: 모델 학습
criterion = nn.CrossEntropyLoss()
optimizer = torch.optim.Adam(model.parameters(), lr=0.001)
num_epochs = 5

# 가중치 저장 경로
save_path = 'model_weights'
os.makedirs(save_path, exist_ok=True)

for epoch in range(num_epochs):
    model.train()
    running_loss = 0.0
    total_correct = 0
    total_samples = 0

    for batch_idx, (images, labels) in enumerate(train_loader):
        images, labels = images.to(device), labels.to(device)

        # 모델 학습
        optimizer.zero_grad()
        outputs = model(images)
        loss = criterion(outputs, labels)
        loss.backward()
        optimizer.step()

        running_loss += loss.item()

        # Accuracy 계산
        _, preds = torch.max(outputs, 1)
        total_correct += (preds == labels).sum().item()
        total_samples += labels.size(0)

        # Index 10마다 손실과 정확도 출력
        if (batch_idx + 1) % 10 == 0:
            current_loss = running_loss / (batch_idx + 1)
            current_accuracy = total_correct / total_samples * 100
            print(f'Epoch {epoch+1}/{num_epochs}, Batch {batch_idx+1}/{len(train_loader)}, '
                  f'Loss: {current_loss:.4f}, Accuracy: {current_accuracy:.2f}%')

        # 배치 사이즈 200마다 모델 가중치 저장
        if (batch_idx + 1) % 200 == 0:
            torch.save(model.state_dict(), os.path.join(save_path, f'model_epoch{epoch+1}_batch{batch_idx+1}.pth'))
            print(f"Saved model weights at Epoch {epoch+1}, Batch {batch_idx+1}")

    # 에포크별 평균 손실 출력
    epoch_loss = running_loss / len(train_loader)
    epoch_accuracy = total_correct / total_samples * 100
    print(f'Epoch {epoch+1}/{num_epochs}, Average Loss: {epoch_loss:.4f}, Average Accuracy: {epoch_accuracy:.2f}%')

# Step 6: 모델 평가
model.eval()
y_true = []
y_pred = []

with torch.no_grad():
    for images, labels in test_loader:
        images = images.to(device)
        outputs = model(images)
        _, preds = torch.max(outputs, 1)
        y_true.extend(labels.numpy())
        y_pred.extend(preds.cpu().numpy())

accuracy = accuracy_score(y_true, y_pred)
precision = precision_score(y_true, y_pred)
recall = recall_score(y_true, y_pred)
f1 = f1_score(y_true, y_pred)

print(f"Accuracy: {accuracy}")
print(f"Precision: {precision}")
print(f"Recall: {recall}")
print(f"F1 Score: {f1}")

import torch
import torch.nn as nn
import torch.optim as optim
from sklearn.metrics import accuracy_score

torch.save(model.state_dict(), 'eyeopen.pth')
